## Spark SQL

### Spark SQLì´ ì‚¬ìš©ë˜ëŠ” ì´ìœ 
```
[Sparkì˜ 3ê°€ì§€ ë°ì´í„° ë¶„ë¥˜]
1) Unstructured Data    : ë¡œê·¸íŒŒì¼, ì´ë¯¸ì§€    (ìŠ¤í‚¤ë§ˆê°€ ë³€ë™ë˜ëŠ” ë°ì´í„°)
2) Semi Structured Data : CSV, JSON, XML  (í–‰ê³¼ ì—´)
3) Structured Data      : ë°ì´í„°ë² ì´ìŠ¤       (í–‰ê³¼ ì—´ + ë°ì´í„° íƒ€ì…)
```
RDDëŠ” ë°ì´í„° ë‚´ë¶€êµ¬ì¡°ë¥¼ ì •ì˜í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œì˜ ì„±ëŠ¥ì´ ê°œë°œì ì˜ì¡´ì ì¸ ê²½í–¥ì´ ìˆë‹¤.   
> ex) join + filter í•  ë•Œ filter ë¨¼ì € í•˜ëŠ”ê²ƒì´ ì…”í”Œë§ í•  ë•Œì˜ ë°ì´í„° ìˆ˜ë¥¼ ì¤„ì´ê¸° ë•Œë¬¸ì— í¼í¬ë¨¼ìŠ¤ê°€ ì¢‹ë‹¤ëŠ” ê±¸ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë“±ë“±

ë°˜ë©´, êµ¬ì¡°í™”ëœ ë°ì´í„°ì—ì„  êµ¬ì¡°ë¥¼ ì´ë¯¸ ì•Œê³  ìˆê¸° ë•Œë¬¸ì— ì–´ë–¤ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ê²ƒì¸ì§€ ì •ì˜ë§Œ í•˜ë©´ ë˜ë©°, ì„±ëŠ¥ ìµœì í™”ë„ ìë™ìœ¼ë¡œ ìˆ˜í–‰ëœë‹¤.   
Spark SQLì€ ì´ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¥¼ ë‹¤ë£° ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.   
:star2: ê²°ë¡ ì ìœ¼ë¡œ, Spark SQLì€ ë°ì´í„°ë¥¼ êµ¬ì¡°í™”í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨ ê°œë°œìê°€ ì„±ëŠ¥ ë•Œë¬¸ì— ê³ ë¯¼í•˜ëŠ” ë¶€ë‹´ì„ ì¤„ì—¬ì£¼ê¸° ìœ„í•´ ì‚¬ìš©ëœë‹¤.

### Spark SQLì„ ìì„¸íˆ ì•Œì•„ë³´ì
```
[Spark SQLì˜ ì£¼ìš” API]
- sql, dataframe, datasets
[Spark SQLì˜ ë°±ì—”ë“œ ì»´í¬ë„ŒíŠ¸]
- catalyst(ì¿¼ë¦¬ ìµœì í™” ì—”ì§„), tungsten(ì‹œë¦¬ì–¼ë¼ì´ì €)
```
Spark ìœ„ì— êµ¬í˜„ëœ í•˜ë‚˜ì˜ íŒ¨í‚¤ì§€ì´ë‹¤.   
Spark Coreì˜ RDD :arrow_right: Spark SQLì˜DataFrame   
Spark Coreì˜ SparkContext :arrow_right: Spark SQLì˜ SparkSession   
DataFrameì€ í…Œì´ë¸” ë°ì´í„°ì…‹ìœ¼ë¡œ, ê°œë…ì ìœ¼ë¡œ RDDì— ìŠ¤í‚¤ë§ˆê°€ ì ìš©ëœ ê²ƒì´ë‹¤.   
SparkSessionì€ DataFrameì„ ë§Œë“¤ê¸° ìœ„í•´ í•„ìš”í•œ ì„¸ì…˜ì´ë‹¤.   
```python
# sparksession ë§Œë“¤ê¸°
spark = SparkSession.builder.appName(â€œtest-appâ€).getOrCreate()
```
Datasetì€ Typeì´ ìˆëŠ” Dataframeì´ì§€ë§Œ, PySparkì—ì„œëŠ” íƒ€ì…ì„ ì‹ ê²½ì“°ì§€ ì•Šì•„ë„ ëœë‹¤. 

### DataFrame ë§Œë“¤ê¸°
Dataframeì„ ë§Œë“¤ ë•, RDDì—ì„œ ë§Œë“¤ê¸° CSV, JSON ë“± íŒŒì¼ì—ì„œ ë§Œë“¤ê¸° ì˜ 2ê°€ì§€ ë°©ë²•ì´ ìˆë‹¤.   
ğŸ“Œ RDDì—ì„œ ë§Œë“¤ê¸°   
(1) ìŠ¤í‚¤ë§ˆë¥¼ ìë™ìœ¼ë¡œ ìœ ì¶”í•´ì„œ ë§Œë“¤ê¸°
  ```python
  lines = sc.textFile(â€œtest.csvâ€)
  data = lines.map(lambda x: x.split(â€œ,â€))
  preprocessed = data.map(lambda x: Row(name=x[0], price=int(x[1])))
  df = spark.createDataFrame(preprocessed)
  ```
(2) ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©ìê°€ ì •ì˜í•˜ê¸°
  ```python
  schema = StructType(
	StructField(â€œnameâ€, StringType(), True),
	StructField(â€œpriceâ€, StringType(), True)
  )
  spark.createDataFrame(preprocessed, schema)
  ```

ğŸ“Œ íŒŒì¼ì—ì„œ ë§Œë“¤ê¸°   
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName(â€œtest-appâ€).getOrCreate()
# json
df = spark.read.json(â€œtest.jsonâ€)
# txt
df_txt = spark.read.text(â€œtest.txtâ€)
# csv
df_csv = spark.read.csv(â€œtest.csvâ€)
# parquet
df_pq = spark.read.load(â€œtest.parquetâ€)
```

ğŸ“Œ DataFrameì„ í•˜ë‚˜ì˜ tableì²˜ëŸ¼ ë§Œë“¤ê¸°(temporary viewë¥¼ ë§Œë“¤ê¸°)
```python
df.createOrReplaceTempView(â€œtbl_nameâ€)
spark.sql(â€œSELECT col1 FROM tbl_name LIMIT 5â€).show()
```

### Spark SQL Query ì‘ì„±í•˜ê¸°
Spark SQLì€ Hive Query ì™€ ê±°ì˜ ë™ì¼í•˜ë‹¤.   
.sql()ë¡œ SQLë¬¸ì„ ì‚¬ìš©í•˜ê±°ë‚˜, í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ì—¬ ì¿¼ë¦¬ê°€ ê°€ëŠ¥í•˜ë‹¤.   
dataframeì„ RDDë¥¼ ë³€í™˜í•  ìˆ˜ë„ ìˆì§€ë§Œ, MLLibì´ë‚˜ Spark Streamingê³¼ ê°™ì€ ìŠ¤íŒŒí¬ ëª¨ë“ˆì€ dataframeì´ ë” í¸í•˜ê¸° ë•Œë¬¸ì— ê¶Œì¥í•˜ì§„ ì•ŠëŠ”ë‹¤.   
:star2: DataFrameì€ Spark SQLì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° êµ¬ì¡°ì´ë©°, ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ì¿¼ë¦¬ë¥¼ ì´ìš©í•œë‹¤. ë‹¤ë¥¸ ìŠ¤íŒŒí¬ ëª¨ë“ˆê³¼ í˜¸í™˜ì´ ì˜ ë˜ë©°, ë‹¤ë£¨ê¸° ì‰½ê³ , ì„±ëŠ¥ ìµœì í™”ë„ ìë™ìœ¼ë¡œ í•´ì£¼ê¸° ë•Œë¬¸ì— RDDë³´ë‹¤ ë” ë§ì´ ì‚¬ìš©í•˜ê³  ìˆë‹¤.


### DataFrame íŠ¹ì§•
ê´€ê³„í˜• ë°ì´í„°ì…‹ì´ë‹¤. (rdd+relation)    
ìŠ¤í‚¤ë§ˆë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— ìŠ¤í‚¤ë§ˆë¥¼ í†µí•´ ìë™ ìµœì í™” ëœë‹¤.   
ë‚´ë¶€ì ìœ¼ë¡œ íƒ€ì…ì„ ê°•ì œí•˜ì§€ ì•Šì•„ íƒ€ì…ì´ ì—†ë‹¤.   
RDDì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì§€ì—°ì‹¤í–‰(lazy execution)ë˜ë©°, ë¶„ì‚° ì €ì¥ë˜ê³ , immutable(ë¶ˆë³€) í•˜ë‹¤.   
í–‰(row)ê°ì²´ê°€ ìˆë‹¤.   
sql ì¿¼ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.   
csv, json, hive ë“±ìœ¼ë¡œ ì½ì–´ì˜¤ê±°ë‚˜ ë³€í™˜ ê°€ëŠ¥í•˜ë‹¤.   

### DataFrame ì—°ì‚°
- ìŠ¤í‚¤ë§ˆ í™•ì¸í•˜ê¸°
  - dtypes : ìŠ¤í‚¤ë§ˆ êµ¬ì„± ì¶œë ¥
  - show() : í…Œì´ë¸” í˜•íƒœë¡œ ë°ì´í„° ì¶œë ¥
  - printSchema() : íŠ¸ë¦¬ í˜•íƒœë¡œ ìŠ¤í‚¤ë§ˆ ì¶œë ¥

- ë³µì¡í•œ ë°ì´í„° íƒ€ì…
  - ArrayType
  - MapType
  - StructType : object

- sql ê³¼ ë¹„ìŠ·í•œ ì‘ì—…
  - select()
  - agg() : ê·¸ë£¹í•‘ í›„ ì—°ì‚°
  ```python
  df.agg({"age":"max"}).collect()
  >> [Row(max(age)=5] #age ì»¬ëŸ¼ì˜ max ê°’ì´ 5ì´ë‹¤
  ```
  - groupBy() : ì§€ì • ì»¬ëŸ¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í•‘
  ```python
  df.groupBy("name").agg({"age":"mean"}).collect()
  >> [Row(name="Alice", avg(age)=2.0), Row(name="Bob", avg(age)=5.0)] # name ì»¬ëŸ¼ë³„ age ì»¬ëŸ¼ì˜ í‰ê·  ê°’ì€ xì´ë‹¤
  ```
  - join()
  ```python
  df.join(df2, "name").select(df.name, df2.height).collect()
  >> [Row(name="Bob", height=180)]
  ```

### Spark SQL ì˜ ìµœì í™” ì—”ì§„
ìŠ¤íŒŒí¬ëŠ” ì¿¼ë¦¬ë¥¼ ëŒë¦¬ê¸° ìœ„í•´ ë‘ê°€ì§€ ì—”ì§„ì„ ì‚¬ìš©í•œë‹¤.
```
1. Catalyst : ì‚¬ìš©ìê°€ ì“´ ì½”ë“œë¥¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íšìœ¼ë¡œ ë°”ê¾¸ëŠ” ì—”ì§„ìœ¼ë¡œ, ìµœì í™”ëœ ì‹¤í–‰ í”Œëœì„ ìƒì„±í•œë‹¤. 
2. Tungsten : ìµœì í™”ëœ ì½”ë“œë¥¼ row level (ë©”ëª¨ë¦¬, cpu) ì—ì„œ ìµœëŒ€ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ë³€í™˜í•´ì£¼ëŠ” ì—”ì§„ì´ë‹¤.
```
Catalyst ëŠ” Spark SQLê³¼ DataFrameì„ ë™ì‹œì— ë‹¤ë£° ìˆ˜ ìˆëŠ” ëª¨ë“ˆì´ë‹¤.   
ì£¼ ì—­í• ì€ Logical Planì„ Physical Plan ìœ¼ë¡œ ë°”ê¾¸ëŠ” ì¼ì´ë‹¤.   
ìœ„ì™€ ê°™ì€ ê³¼ì •ì€ ìŠ¤íŒŒí¬ ì—”ì§„ì˜ ì„±ëŠ¥ í–¥ìƒì´ ì£¼ìš” ëª©ì ì´ë‹¤. (ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”, ìºì‹œ í™œìš© ì—°ì‚°, ì½”ë“œ ìƒì„±)
> Logical Plan : ìˆ˜í–‰í•´ì•¼ í•˜ëŠ” ëª¨ë“  transformation ë‹¨ê³„ì— ëŒ€í•œ ì¶”ìƒí™”ì´ë‹¤. ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë³€í•´ì•¼ í•˜ëŠ”ì§€ ì •ì˜í•˜ì§€ë§Œ, ì‹¤ì œ ì–´ë””ì„œ ì–´ë–»ê²Œ ë™ì‘í•˜ëŠ”ì§€ëŠ” ì •ì˜í•˜ì§€ ì•ŠëŠ”ë‹¤.   
> Physical Plan : Logical Planì´ í´ëŸ¬ìŠ¤í„° ìœ„ì—ì„œ ì–´ë–»ê²Œ ì‹¤í–‰ë ì§€ë¥¼ ì •ì˜í•œë‹¤. ì‹¤í–‰ ì „ëµì„ ë§Œë“¤ê³  cost model ì— ë”°ë¼ ìµœì í™”í•œë‹¤.

### Catalyst ì˜ Logical->Physical Plan ë³€ê²½í•˜ëŠ” ê³¼ì •
<img width="1300" alt="image" src="https://user-images.githubusercontent.com/28644251/163387183-d385d60f-a343-4509-9f76-e2596b2c2dec.png">

1. ë¶„ì„ : DataFrame ì˜ relationì„ ê³„ì‚°í•˜ê³ , ì»¬ëŸ¼ íƒ€ì…ê³¼ ì´ë¦„ì„ í™•ì¸í•œë‹¤. (ì»¬ëŸ¼, í…Œì´ë¸” ì˜¤íƒ€ê°€ ìˆìœ¼ë©´ ì—ëŸ¬ ë‚¨)    
2. Logical Plan ìµœì í™”  
	2-1. ìƒìˆ˜ë¡œ í‘œí˜„ëœ í‘œí˜„ì‹ì„ ì»´íŒŒì¼ íƒ€ì„ì— ê³„ì‚° (ëŸ°íƒ€ì„ x)   
	2-2. predicate pushdown. join&filter -> filter&join ì²˜ëŸ¼ ì…”í”Œë§ ê´€ë ¨ëœ ì‘ì—… ì „ íš¨ìœ¨ì ì¸ ì‘ì—…ì„ í•˜ë„ë¡ ìµœì í™”   
	2-3. projection pruning. ì—°ì‚°ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ê°€ì ¸ì˜¤ê¸°   
3. Physical Plan : ìŠ¤íŒŒí¬ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ í”Œëœìœ¼ë¡œ ë³€í™˜   
4. Code Generation : ìµœì í™”ëœ Physical Plan ì„ java bytecode ë¡œ ë³€í™˜   

### ë‹¨ê³„ë³„ Planì„ ì‚´í´ë³´ëŠ” í•¨ìˆ˜ explain()
```python
# í”¼ì§€ì»¬, ë¡œì§€ì»¬ í”Œëœ ëª¨ë‘ ì¡°íšŒ
spark.sql(query).explain(True)
'''
- parsed logical plan    : ì‚¬ìš©ìê°€ ì‘ì„±í•œ ì¿¼ë¦¬ë¥¼ ê·¸ëŒ€ë¡œ ì¡°íšŒ
- analyzed logical plan  : ì‚¬ìš©ìê°€ ì§€ì •í•œ í…Œì´ë¸”ì˜ ì»¬ëŸ¼ êµ¬ì„± ì¡°íšŒ
- optimized logical plan : ìµœì í™”ëœ ì¿¼ë¦¬ë¥¼ ì¡°íšŒ
- physical plan          : ìµœì í™”ëœ ë¡œì§€ì»¬ í”Œëœì´ ì‹¤ì œë¡œ ì–´ë–»ê²Œ ë™ì‘í• ì§€ ìì„¸í•œ ê³„íšì„ ì¡°íšŒ (ì¡°ì¸ ì¢…ë¥˜ê¹Œì§€ ì •ì˜)
'''
``` 
	
-------------
### Practice
- Spark SQL Query ì‘ì„± : [learn_sql.jpynb](https://github.com/Jiyongs/dev_study/blob/master/bigdata/learn_sql.ipynb)
- Spark DataFrame ë‹¤ë£¨ê¸° : [dataframe_prac](https://github.com/Jiyongs/dev_study/blob/master/bigdata/dataframe_prac.ipynb)
- Spark DataFrame SQL ì˜ˆì œ ì‹¤ìŠµ : [trip_count_sql](https://github.com/Jiyongs/dev_study/blob/master/bigdata/trip_count_sql.ipynb)

### Reference
- 'ì‹¤ì‹œê°„ ë¹…ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ Spark & Flink Oline' ê°•ì˜ (Part 3)

### ì£¼ì˜ì‚¬í•­
- ì£¼í”¼í„° ë…¸íŠ¸ë¶ì€ íŒŒì´ì¬ í”„ë¡œì íŠ¸ ê²½ë¡œë¡œ ì´ë™ í•œ í›„ í„°ë¯¸ë„ì— 'jupyter notebook' ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì—¬ ì‹¤í–‰í•œë‹¤.
- SparkSession getOrCreate() ì‚¬ìš© ì‹œ ì˜¤ë¥˜
  ``` python
  java.net.BindException: Can't assign requested address: Service 'sparkDriver' failed after 16 retries (on a random free port)!
  ``` 
  > 1) í„°ë¯¸ë„ì—ì„œ export SPARK_LOCAL_IP="127.0.0.1" ì‹¤í–‰ 
  > 2) /usr/local/Cellar/apache-spark/3.2.1/libexec/conf/spark-env.sh ì—ì„œ SPARK_LOCAL_IP=127.0.0.1 ì„¤ì •

